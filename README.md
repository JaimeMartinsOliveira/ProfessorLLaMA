
# ü§ñ Chatbot com LLaMA + FastAPI

Este projeto √© uma **API de chatbot** constru√≠da com **FastAPI**, utilizando um **modelo de linguagem (LLM)** como o LLaMA ou Mistral, via [Transformers](https://huggingface.co/docs/transformers/) da Hugging Face, com suporte √† quantiza√ß√£o 4-bit por meio do `bitsandbytes`. Ideal para integra√ß√£o com Web, WhatsApp, aplicativos ou outras interfaces.

---

## üì¶ Requisitos

- Python 3.10+ 
- CUDA (opcional, para uso com GPU NVIDIA)
- PyTorch
- transformers 
- bitsandbytes
- accelerate
- uvicorn
- fastapi

### üì• Instala√ß√£o

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Configura√ß√£o do Modelo

Este projeto utiliza um modelo LLaMA, Mistral ou similar, com quantiza√ß√£o 4-bit para reduzir o uso de mem√≥ria.

### üîπ Op√ß√£o 1: Carregar totalmente na CPU (mais compat√≠vel, por√©m mais lento)

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map={"": "cpu"}
)
```

### üîπ Op√ß√£o 2: Offload entre GPU e CPU (exige `accelerate` configurado)

```python
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    llm_int8_enable_fp32_cpu_offload=True
)
```

> üîó Consulte a [documenta√ß√£o oficial da Hugging Face](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu) para mais detalhes sobre o offload.

---

## üöÄ Executando a API

Inicie o servidor local com:

```bash
uvicorn main:app --reload
```

A API estar√° dispon√≠vel em: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## üîç Testando a API

Ap√≥s iniciar, acesse a documenta√ß√£o interativa:

- Swagger UI: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)
- ReDoc: [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)

### Exemplo de requisi√ß√£o `POST`

```http
POST /chat
Content-Type: application/json

{
  "message": "Ol√°, tudo bem?"
}
```

---

## üí¨ Integra√ß√µes poss√≠veis

A API pode ser usada com diferentes front-ends e servi√ßos:

- Interfaces Web com **Gradio** ou **Streamlit**
- Bots de WhatsApp usando **Twilio** ou **Venom Bot**
- Aplicativos m√≥veis com **Flutter**, **React Native** ou **Ionic**

---

## üß† Modelos Compat√≠veis

Voc√™ pode alterar o modelo via Hugging Face Hub. Exemplos:

```python
model_id = "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
```

Outros modelos recomendados:

- `meta-llama/Llama-2-7b-chat-hf`
- `mistralai/Mistral-7B-Instruct-v0.2`
- `tiiuae/falcon-7b-instruct`
- `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (leve e r√°pido)

---

## üõ†Ô∏è Boas pr√°ticas e dicas

- Prefira modelos quantizados (`4bit`, `8bit`) para economizar mem√≥ria.
- Use `device_map={"": "cpu"}` se n√£o tiver GPU ou estiver enfrentando erros de mem√≥ria.
- Certifique-se de que `accelerate` esteja instalado e atualizado.
- Para notebooks com at√© **6GB de VRAM**, √© altamente recomendado usar quantiza√ß√£o com offload para CPU.
